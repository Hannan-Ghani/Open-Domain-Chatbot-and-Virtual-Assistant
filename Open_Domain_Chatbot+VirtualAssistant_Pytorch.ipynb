{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4893f120",
      "metadata": {
        "id": "4893f120",
        "outputId": "a40ed4bc-d782-484d-c020-cfedd8c45a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: speechrecognition in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (3.8.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install speechrecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e794c4b3",
      "metadata": {
        "id": "e794c4b3",
        "outputId": "c5844745-ee65-4dd9-e34a-a207005f4bdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyttsx3 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (2.90)\n",
            "Requirement already satisfied: pypiwin32 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pyttsx3) (223)\n",
            "Requirement already satisfied: pywin32 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pyttsx3) (302)\n",
            "Requirement already satisfied: comtypes in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pyttsx3) (1.1.14)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pyttsx3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f04e295",
      "metadata": {
        "id": "4f04e295",
        "outputId": "a7ec6c39-add2-4ffd-a100-0bb0a4783012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pywhatkit in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (5.4)\n",
            "Requirement already satisfied: requests in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pywhatkit) (2.28.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pywhatkit) (9.2.0)\n",
            "Requirement already satisfied: wikipedia in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pywhatkit) (1.4.0)\n",
            "Requirement already satisfied: pyautogui in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pywhatkit) (0.9.53)\n",
            "Requirement already satisfied: Flask in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pywhatkit) (2.2.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from Flask->pywhatkit) (2.2.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from Flask->pywhatkit) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from Flask->pywhatkit) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from Flask->pywhatkit) (4.12.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from Flask->pywhatkit) (2.1.2)\n",
            "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pyautogui->pywhatkit) (0.0.9)\n",
            "Requirement already satisfied: mouseinfo in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pyautogui->pywhatkit) (0.1.3)\n",
            "Requirement already satisfied: pymsgbox in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pyautogui->pywhatkit) (1.0.9)\n",
            "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pyautogui->pywhatkit) (0.1.28)\n",
            "Requirement already satisfied: PyTweening>=1.0.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pyautogui->pywhatkit) (1.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->pywhatkit) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->pywhatkit) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->pywhatkit) (1.26.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->pywhatkit) (3.3)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from wikipedia->pywhatkit) (4.11.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from click>=8.0->Flask->pywhatkit) (0.4.5)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from importlib-metadata>=3.6.0->Flask->pywhatkit) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from Jinja2>=3.0->Flask->pywhatkit) (2.1.1)\n",
            "Requirement already satisfied: pyrect in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui->pywhatkit) (0.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from beautifulsoup4->wikipedia->pywhatkit) (2.3.1)\n",
            "Requirement already satisfied: pyperclip in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from mouseinfo->pyautogui->pywhatkit) (1.8.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pywhatkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "344a28c3",
      "metadata": {
        "id": "344a28c3",
        "outputId": "9462b322-1f53-4bec-d650-c676245be308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyjokes in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (0.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pyjokes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4256f6",
      "metadata": {
        "id": "3a4256f6",
        "outputId": "41057eaf-a50f-4ab4-d313-1f9238405076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (4.21.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (1.23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (2022.8.17)\n",
            "Requirement already satisfied: requests in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (0.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (21.3)\n",
            "\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->transformers) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a211ab6d",
      "metadata": {
        "id": "a211ab6d",
        "outputId": "d6c52843-ca30-4a89-dd40-385aca733387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (0.13.1)\n",
            "Requirement already satisfied: torch==1.12.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from torchvision) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from torchvision) (1.23.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from torchvision) (9.2.0)\n",
            "Requirement already satisfied: requests in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from torchvision) (2.28.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->torchvision) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->torchvision) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests->torchvision) (2022.6.15)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad11428",
      "metadata": {
        "scrolled": true,
        "id": "4ad11428",
        "outputId": "5ca4623b-71d6-4c5b-d2f2-685c6206d424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.12-cp39-cp39-win_amd64.whl (163 kB)\n",
            "     -------------------------------------- 164.0/164.0 kB 3.3 MB/s eta 0:00:00\n",
            "Installing collected packages: pyaudio\n",
            "Successfully installed pyaudio-0.2.12\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pyaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e1d3d7",
      "metadata": {
        "id": "94e1d3d7",
        "outputId": "4577ba81-b2db-481b-c5d5-b7c3bf4aa39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandasNote: you may need to restart the kernel to use updated packages.\n",
            "  Downloading pandas-1.4.3-cp39-cp39-win_amd64.whl (10.6 MB)\n",
            "     --------------------------------------- 10.6/10.6 MB 11.5 MB/s eta 0:00:00\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\n",
            "     ------------------------------------- 500.6/500.6 kB 15.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from pandas) (1.23.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Installing collected packages: pytz, pandas\n",
            "\n",
            "Successfully installed pandas-1.4.3 pytz-2022.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "430e734b",
      "metadata": {
        "id": "430e734b",
        "outputId": "6f6a1942-ee0d-48cd-fdec-e140b3d8bf1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.1.2-cp39-cp39-win_amd64.whl (7.4 MB)\n",
            "     ---------------------------------------- 7.4/7.4 MB 10.8 MB/s eta 0:00:00\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.9.0-cp39-cp39-win_amd64.whl (38.6 MB)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "     --------------------------------------- 38.6/38.6 MB 10.9 MB/s eta 0:00:00\n",
            "Collecting joblib>=1.0.0\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "     -------------------------------------- 307.0/307.0 kB 9.6 MB/s eta 0:00:00\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from scikit-learn) (1.23.2)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.1.0 scikit-learn-1.1.2 scipy-1.9.0 threadpoolctl-3.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c856453",
      "metadata": {
        "id": "3c856453",
        "outputId": "c5d33055-0d3f-4db5-9b54-298a159335e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
            "     ---------------------------------------- 5.9/5.9 MB 11.4 MB/s eta 0:00:00\n",
            "Collecting absl-py>=0.4\n",
            "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
            "     -------------------------------------- 123.4/123.4 kB 7.1 MB/s eta 0:00:00\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\n",
            "     ------------------------------------- 167.2/167.2 kB 10.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from tensorboard) (0.37.1)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "     ---------------------------------------- 93.3/93.3 kB ? eta 0:00:00\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from tensorboard) (2.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from tensorboard) (63.4.1)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "     ------------------------------------- 781.3/781.3 kB 12.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from tensorboard) (1.23.2)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from tensorboard) (2.28.1)\n",
            "Collecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.47.0-cp39-cp39-win_amd64.whl (3.6 MB)\n",
            "     ---------------------------------------- 3.6/3.6 MB 11.3 MB/s eta 0:00:00\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.4-cp39-cp39-win_amd64.whl (895 kB)\n",
            "     -------------------------------------- 895.7/895.7 kB 8.1 MB/s eta 0:00:00\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "     -------------------------------------- 155.3/155.3 kB 9.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\hanna\\.conda\\envs\\chatbot_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.1)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "     ---------------------------------------- 77.1/77.1 kB 4.2 MB/s eta 0:00:00\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "     -------------------------------------- 151.5/151.5 kB 8.8 MB/s eta 0:00:00\n",
            "Installing collected packages: tensorboard-plugin-wit, pyasn1, tensorboard-data-server, rsa, pyasn1-modules, protobuf, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboardNote: you may need to restart the kernel to use updated packages.\n",
            "Successfully installed absl-py-1.2.0 cachetools-5.2.0 google-auth-2.11.0 google-auth-oauthlib-0.4.6 grpcio-1.47.0 markdown-3.4.1 oauthlib-3.2.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c38e139",
      "metadata": {
        "id": "3c38e139"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "import pywhatkit\n",
        "import datetime\n",
        "import wikipedia\n",
        "import pyjokes\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        " \n",
        "import winsound\n",
        "frequency = 4400  # Set Frequency To 4400 Hertz\n",
        "duration = 200  # Set Duration To 200 ms == 0.2 seconds\n",
        "\n",
        "\n",
        "\n",
        "print(\"Initialising the Model 1/2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "print(\"Initialising the Model 2/2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "print(\"Initialising done!\")\n",
        "\n",
        "r = sr.Recognizer()\n",
        "engine = pyttsx3.init()\n",
        "voices = engine.getProperty('voices')\n",
        "engine.setProperty('voice', voices[0].id)\n",
        "\n",
        "def talk(text):\n",
        "    engine.say(text)\n",
        "    engine.runAndWait()\n",
        "\n",
        "def takecommand():\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Listening!\")\n",
        "        audio = r.listen(source)\n",
        "        print(\"Audio Captured\")\n",
        "\n",
        "    try:\n",
        "        command = r.recognize_google(audio)\n",
        "        command = command.lower()\n",
        "        if 'jarvis' in command:\n",
        "            command = command.replace('jarvis', '')\n",
        "            \n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Jarvis could not understand audio\")\n",
        "        command = \"nothing\"\n",
        "    except sr.RequestError as e:\n",
        "        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
        "        command = \"nothing\"\n",
        "    return command\n",
        "\n",
        "def run_jarvis():\n",
        "    command = takecommand()\n",
        "    print(command)\n",
        "    if command == \"nothing\":\n",
        "        print(\"Could'nt hear you try again!\")\n",
        "    elif command == 'false':\n",
        "        print(\"Say Jarvis to start a command!\")\n",
        "    elif command == 'stop':\n",
        "        exit()\n",
        "    elif 'play' in command:\n",
        "        song = command.replace('play','')\n",
        "        talk(\"playing \" + song)\n",
        "        pywhatkit.playonyt(song)\n",
        "    elif 'time' in command:\n",
        "        time = datetime.datetime.now().strftime('%I:%M %p')\n",
        "        talk('Current time is ' + time)\n",
        "    elif 'tell me about' in command:\n",
        "        person = command.replace('tell me about', '')\n",
        "        try:\n",
        "            info = wikipedia.summary(person, 1)\n",
        "            print(info)\n",
        "            talk(info)\n",
        "        except:\n",
        "            print(\"wikipedia coud not process the query\")\n",
        "    elif 'joke' in command:\n",
        "        joke = pyjokes.get_joke()\n",
        "        print(joke)\n",
        "        talk(joke)\n",
        "    else:\n",
        "        new_user_input_ids = tokenizer.encode(command + tokenizer.eos_token, return_tensors='pt')\n",
        "        bot_input_ids = torch.cat([new_user_input_ids], dim=-1)\n",
        "        chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "        text = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "        print(text)\n",
        "        talk(text)\n",
        "\n",
        "winsound.Beep(frequency, duration)\n",
        "talk(\"Hello Iam Jarvis!, Please give a command following the buzz-word Jarvis\")\n",
        "print(\"starting the assistant\")\n",
        "while True:\n",
        "    run_jarvis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da150746",
      "metadata": {
        "id": "da150746"
      },
      "outputs": [],
      "source": [
        "run_jarvis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ada4b6",
      "metadata": {
        "id": "d8ada4b6"
      },
      "outputs": [],
      "source": [
        "# all the imports\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import (\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0862df15",
      "metadata": {
        "id": "0862df15"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2e0981",
      "metadata": {
        "id": "8d2e0981"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('Downloads/RickAndMortyScripts.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ae68f0",
      "metadata": {
        "id": "90ae68f0"
      },
      "outputs": [],
      "source": [
        "data.sample(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cec070f",
      "metadata": {
        "id": "8cec070f"
      },
      "outputs": [],
      "source": [
        "CHARACTER_NAME = 'Morty'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31f4369b",
      "metadata": {
        "id": "31f4369b"
      },
      "outputs": [],
      "source": [
        "contexted = []\n",
        "\n",
        "# context window of size 7\n",
        "n = 7\n",
        "\n",
        "for i in data[data.name == CHARACTER_NAME].index:\n",
        "  if i < n:\n",
        "    continue\n",
        "  row = []\n",
        "  prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n",
        "  for j in range(i, prev, -1):\n",
        "    row.append(data.line[j])\n",
        "  contexted.append(row)\n",
        "\n",
        "columns = ['response', 'context'] \n",
        "columns = columns + ['context/' + str(i) for i in range(n - 1)]\n",
        "\n",
        "df = pd.DataFrame.from_records(contexted, columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68bc1aa",
      "metadata": {
        "id": "e68bc1aa"
      },
      "outputs": [],
      "source": [
        "df.sample(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc0ba002",
      "metadata": {
        "id": "cc0ba002"
      },
      "outputs": [],
      "source": [
        "trn_df, val_df = train_test_split(df, test_size=0.1)\n",
        "trn_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c9b8d4",
      "metadata": {
        "id": "01c9b8d4"
      },
      "outputs": [],
      "source": [
        "# create dataset suitable for our model\n",
        "def construct_conv(row, tokenizer, eos = True):\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
        "    conv = flatten(conv)\n",
        "    return conv\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
        "\n",
        "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
        "\n",
        "        directory = args.cache_dir\n",
        "        cached_features_file = os.path.join(\n",
        "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"rb\") as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            for _, row in df.iterrows():\n",
        "                conv = construct_conv(row, tokenizer)\n",
        "                self.examples.append(conv)\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"wb\") as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "567aafd2",
      "metadata": {
        "id": "567aafd2"
      },
      "outputs": [],
      "source": [
        "# Cacheing and storing of data/checkpoints\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
        "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
        "    ordering_and_checkpoint_path = []\n",
        "\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
        "\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    return checkpoints_sorted\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
        "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "344a3865",
      "metadata": {
        "id": "344a3865"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca60830",
      "metadata": {
        "id": "bca60830"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "# Configs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d077efc",
      "metadata": {
        "id": "3d077efc"
      },
      "outputs": [],
      "source": [
        "# Args to allow for easy convertion of python script to notebook\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.output_dir = 'output-small'\n",
        "        self.model_type = 'gpt2'\n",
        "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
        "        self.config_name = 'microsoft/DialoGPT-small'\n",
        "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
        "        self.cache_dir = 'cached'\n",
        "        self.block_size = 512\n",
        "        self.do_train = True\n",
        "        self.do_eval = True\n",
        "        self.evaluate_during_training = False\n",
        "        self.per_gpu_train_batch_size = 4\n",
        "        self.per_gpu_eval_batch_size = 4\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.learning_rate = 5e-5\n",
        "        self.weight_decay = 0.0\n",
        "        self.adam_epsilon = 1e-8\n",
        "        self.max_grad_norm = 1.0\n",
        "        self.num_train_epochs = 4\n",
        "        self.max_steps = -1\n",
        "        self.warmup_steps = 0\n",
        "        self.logging_steps = 1000\n",
        "        self.save_steps = 3500\n",
        "        self.save_total_limit = None\n",
        "        self.eval_all_checkpoints = False\n",
        "        self.no_cuda = True\n",
        "        self.overwrite_output_dir = True\n",
        "        self.overwrite_cache = True\n",
        "        self.should_continue = False\n",
        "        self.seed = 42\n",
        "        self.local_rank = -1\n",
        "        self.fp16 = False\n",
        "        self.fp16_opt_level = 'O1'\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe1a2ee",
      "metadata": {
        "id": "5fe1a2ee"
      },
      "outputs": [],
      "source": [
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    # add_special_tokens_(model, tokenizer)\n",
        "\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if (\n",
        "        args.model_name_or_path\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            inputs, labels = (batch, batch)\n",
        "            if inputs.shape[1] > 1024: continue\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if (\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = \"checkpoint\"\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "                    os.makedirs(output_dir, exist_ok=True)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "# Evaluation of some model\n",
        "\n",
        "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
        "    os.makedirs(eval_output_dir, exist_ok=True)\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        inputs, labels = (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\"perplexity\": perplexity}\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0327dd47",
      "metadata": {
        "id": "0327dd47"
      },
      "outputs": [],
      "source": [
        "# Main runner\n",
        "\n",
        "def main(df_trn, df_val):\n",
        "    args = Args()\n",
        "    \n",
        "    if args.should_continue:\n",
        "        sorted_checkpoints = _sorted_checkpoints(args)\n",
        "        if len(sorted_checkpoints) == 0:\n",
        "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
        "        else:\n",
        "            args.model_name_or_path = sorted_checkpoints[-1]\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "        and not args.should_continue\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    device = torch.device(\"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    model = AutoModelWithLMHead.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        from_tf=False,\n",
        "        config=config,\n",
        "        cache_dir=args.cache_dir,\n",
        "    )\n",
        "    model.to(args.device)\n",
        "    \n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
        "\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    if args.do_train:\n",
        "        # Create output directory if needed\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = (\n",
        "            model.module if hasattr(model, \"module\") else model\n",
        "        )  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1492fb",
      "metadata": {
        "id": "3f1492fb",
        "outputId": "8ff1e03c-76db-4888-c1e9-28944ad2baa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d8a0018",
      "metadata": {
        "id": "2d8a0018"
      },
      "outputs": [],
      "source": [
        "main(trn_df, val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4f45b2",
      "metadata": {
        "id": "6e4f45b2"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import IntProgress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68502add",
      "metadata": {
        "id": "68502add"
      },
      "outputs": [],
      "source": [
        "pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c586cf4",
      "metadata": {
        "id": "1c586cf4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db2477b8",
      "metadata": {
        "id": "db2477b8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}